{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"hello world\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from pandas import ExcelFile\n",
    "from pandas import ExcelWriter\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juke\\Documents\\GitHub\\Zachs_Repo\n"
     ]
    }
   ],
   "source": [
    "rootdir = 'C:/Users/Zman/Desktop/Jiook/code_embarc/data/connectome_fmri'\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1060 137\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "\n",
    "fname_1 = 'all_fmri_pat.xlsx'\n",
    "\n",
    "def get_list():\n",
    "    corr_mats = np.array()\n",
    "    names = []\n",
    "    labels = []\n",
    "    xl_workbook = xlrd.open_workbook(fname_1)\n",
    "    sheet_names = xl_workbook.sheet_names()\n",
    "    xl_sheet = xl_workbook.sheet_by_index(0)\n",
    "    print(xl_sheet.cell_value(1, 1))\n",
    "    print(xl_sheet.ncols, xl_sheet.nrows)\n",
    "    for i in range(xl_sheet.nrows):\n",
    "        if (xl_sheet.cell(i, 1).value == 0 or xl_sheet.cell(i, 1).value == 1):\n",
    "            names.append(xl_sheet.cell_value(i, 0))\n",
    "            labels.append(xl_sheet.cell_value(i, 3))\n",
    "    \n",
    "    return names, labels\n",
    "    #inds = [(i for i in col_1 if (col_1[i]))]\n",
    "    \n",
    "names, labels = get_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classifier(features, labels): # a simple classifier that when given a bunch of correlation matrices and labels, will predict label on new matrices.\n",
    "    train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.3, random_state=42) # splits features w/ labels into training and testing sets\n",
    "    gnb = GaussianNB()\n",
    "    model = gnb.fit(train, train_labels)\n",
    "    preds = model.pred(test, test_labels)\n",
    "    print(accuracy_score(test_labels, preds))\n",
    "    correct = []\n",
    "    for n in len(preds):\n",
    "        if (preds[n] == test_labels[n]):\n",
    "            correct.append(1)\n",
    "        else :\n",
    "            correct.append(0)\n",
    "    return correct, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1060 137\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['CU0046' 'TX0043' 'UM0073' 'CU0117' 'MG0172' 'CU0027' 'UM0118' 'TX0142'\n 'MG0152' 'MG0101' 'CU0074' 'TX0067' 'MG0076' 'CU0034' 'MG0218' 'TX0095'\n 'TX0112' 'CU0115' 'CU0083' 'TX0193' 'UM0047' 'CU0106' 'TX0110' 'MG0270'\n 'CU0066' 'CU0104' 'CU0024' 'CU0077' 'MG0135' 'CU0036' 'UM0089' 'CU0033'\n 'TX0101' 'UM0110' 'TX0189' 'TX0050' 'TX0014' 'UM0103' 'MG0243' 'TX0127'\n 'TX0083' 'UM0119' 'TX0188' 'MG0182' 'TX0070' 'MG0257' 'UM0048' 'TX0068'\n 'TX0136' 'MG0021' 'UM0090' 'TX0078' 'TX0090' 'UM0037' 'MG0126' 'CU0119'\n 'UM0060' 'CU0021' 'TX0039' 'CU0090' 'CU0022' 'CU0103' 'UM0025' 'UM0074'\n 'TX0133' 'TX0195' 'TX0187' 'CU0084' 'TX0079' 'TX0115' 'CU0067' 'UM0052'\n 'TX0020' 'UM0107'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a4d3acdf6930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# gives \"sureness\" of prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# problematic because this only has 33% of data. I want to record accuracy throughout process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-621803726a8a>\u001b[0m in \u001b[0;36mbinary_classifier\u001b[1;34m(features, labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# splits features w/ labels into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tfpls\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m    185\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tfpls\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tfpls\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['CU0046' 'TX0043' 'UM0073' 'CU0117' 'MG0172' 'CU0027' 'UM0118' 'TX0142'\n 'MG0152' 'MG0101' 'CU0074' 'TX0067' 'MG0076' 'CU0034' 'MG0218' 'TX0095'\n 'TX0112' 'CU0115' 'CU0083' 'TX0193' 'UM0047' 'CU0106' 'TX0110' 'MG0270'\n 'CU0066' 'CU0104' 'CU0024' 'CU0077' 'MG0135' 'CU0036' 'UM0089' 'CU0033'\n 'TX0101' 'UM0110' 'TX0189' 'TX0050' 'TX0014' 'UM0103' 'MG0243' 'TX0127'\n 'TX0083' 'UM0119' 'TX0188' 'MG0182' 'TX0070' 'MG0257' 'UM0048' 'TX0068'\n 'TX0136' 'MG0021' 'UM0090' 'TX0078' 'TX0090' 'UM0037' 'MG0126' 'CU0119'\n 'UM0060' 'CU0021' 'TX0039' 'CU0090' 'CU0022' 'CU0103' 'UM0025' 'UM0074'\n 'TX0133' 'TX0195' 'TX0187' 'CU0084' 'TX0079' 'TX0115' 'CU0067' 'UM0052'\n 'TX0020' 'UM0107'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "features, labels = get_list()\n",
    "corr, mod = binary_classifier(features, labels)\n",
    "m = binary_classifier(features, corr) # gives \"sureness\" of prediction\n",
    "# problematic because this only has 33% of data. I want to record accuracy throughout process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_classifier(features, labels): # if sureness if lower than a threshold, then use classification of new\n",
    "    threshold = 0.5\n",
    "    train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    svc = SVC()\n",
    "    model = svc.fit(train, train_labels)\n",
    "    for i in len(test):\n",
    "        if (m.pred(test[i], test_labels[i]) < threshold):\n",
    "            preds = svc.pred(test[i], test_labels[i])\n",
    "        else:\n",
    "            preds = m.pred(test[i], test_labels[i])\n",
    "    print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_classifier(features, labels))\n",
    "print(new_classifier(features, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
